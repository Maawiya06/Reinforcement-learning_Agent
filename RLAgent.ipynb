{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d8157d-dea2-4136-b23b-76464ad8a870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter grid size (e.g., 5 for a 5x5 grid):  5\n",
      "Enter start position (row,col), e.g., 0,0:  0,0\n",
      "Enter goal position (row,col), e.g., 4,4:  4,4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter obstacle positions (row,col), one per line. Type 'done' when finished.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  1,1\n",
      ">  2,2\n",
      ">  done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n",
      "Episode 100/2000 | Total Reward: 56\n",
      "Episode 200/2000 | Total Reward: 76\n",
      "Episode 300/2000 | Total Reward: 87\n",
      "Episode 400/2000 | Total Reward: 82\n",
      "Episode 500/2000 | Total Reward: 93\n",
      "Episode 600/2000 | Total Reward: 93\n",
      "Episode 700/2000 | Total Reward: 93\n",
      "Episode 800/2000 | Total Reward: 93\n",
      "Episode 900/2000 | Total Reward: 93\n",
      "Episode 1000/2000 | Total Reward: 93\n",
      "Episode 1100/2000 | Total Reward: 93\n",
      "Episode 1200/2000 | Total Reward: 93\n",
      "Episode 1300/2000 | Total Reward: 93\n",
      "Episode 1400/2000 | Total Reward: 93\n",
      "Episode 1500/2000 | Total Reward: 93\n",
      "Episode 1600/2000 | Total Reward: 93\n",
      "Episode 1700/2000 | Total Reward: 93\n",
      "Episode 1800/2000 | Total Reward: 93\n",
      "Episode 1900/2000 | Total Reward: 93\n",
      "Episode 2000/2000 | Total Reward: 93\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Evaluating Agent ---\n",
      "\n",
      "--- Evaluation Episode 1 ---\n",
      "A _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S A _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ A _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X A _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ A _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ A\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ A\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ A\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ A\n",
      "---------\n",
      "Path taken: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Total Reward: 93\n",
      "\n",
      "--- Evaluation Episode 2 ---\n",
      "A _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S A _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ A _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X A _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ A _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ A\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ A\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ A\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ A\n",
      "---------\n",
      "Path taken: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Total Reward: 93\n",
      "\n",
      "--- Evaluation Episode 3 ---\n",
      "A _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S A _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ A _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X A _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ A _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ A\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ A\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ A\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ A\n",
      "---------\n",
      "Path taken: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Total Reward: 93\n",
      "\n",
      "--- Evaluation Episode 4 ---\n",
      "A _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S A _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ A _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X A _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ A _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ A\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ A\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ A\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ A\n",
      "---------\n",
      "Path taken: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Total Reward: 93\n",
      "\n",
      "--- Evaluation Episode 5 ---\n",
      "A _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S A _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ A _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X A _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ A _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ A\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ A\n",
      "_ _ _ _ _\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ A\n",
      "_ _ _ _ G\n",
      "---------\n",
      "S _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ A\n",
      "---------\n",
      "Path taken: [(0, 0), (0, 1), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Total Reward: 93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size=5, start=(0, 0), goal=(4, 4), obstacles=None):\n",
    "        #Enviroment\n",
    "        self.size = size\n",
    "        self.start_pos = start\n",
    "        self.goal_pos = goal\n",
    "        self.obstacles = obstacles if obstacles is not None else []\n",
    "        self.agent_pos = self.start_pos\n",
    "        # Define the action space: 0: up, 1: down, 2: left, 3: right\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "\n",
    "    def reset(self):\n",
    "        #Agent at Starting pos\n",
    "        self.agent_pos = self.start_pos\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        #current state of agent\n",
    "        return self.agent_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = self.agent_pos\n",
    "\n",
    "        # Move the agent based on the action\n",
    "        if action == 0:  # Up\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1:  # Down\n",
    "            row = min(self.size - 1, row + 1)\n",
    "        elif action == 2:  # Left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 3:  # Right\n",
    "            col = min(self.size - 1, col + 1)\n",
    "\n",
    "        next_pos = (row, col)\n",
    "\n",
    "        # Check for collisions with obstacles\n",
    "        if next_pos in self.obstacles:\n",
    "            reward = -10  \n",
    "            done = False\n",
    "            # Agent stays in the same position if it hits an obstacle\n",
    "        else:\n",
    "            self.agent_pos = next_pos\n",
    "            # Check if the goal is reached\n",
    "            if self.agent_pos == self.goal_pos:\n",
    "                reward = 100 \n",
    "                done = True\n",
    "            else:\n",
    "                reward = -1 \n",
    "                done = False\n",
    "\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def render(self):\n",
    "        #print the state\n",
    "        grid = np.full((self.size, self.size), '_', dtype=str)\n",
    "        grid[self.start_pos] = 'S'\n",
    "        grid[self.goal_pos] = 'G'\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs] = 'X'\n",
    "        grid[self.agent_pos] = 'A'\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        print(\"-\" * (2 * self.size - 1))\n",
    "\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_space_size, action_space_size, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995, min_exploration_rate=0.01):\n",
    "        \n",
    "        self.q_table = np.zeros(state_space_size + (action_space_size,))\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = exploration_rate\n",
    "        self.epsilon_decay = exploration_decay\n",
    "        self.min_epsilon = min_exploration_rate\n",
    "        self.actions = list(range(action_space_size))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.actions)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        old_value = self.q_table[state][action]\n",
    "        next_max = np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Q-learning formula\n",
    "        new_value = (1 - self.lr) * old_value + self.lr * (reward + self.gamma * next_max)\n",
    "        self.q_table[state][action] = new_value\n",
    "\n",
    "    def decay_exploration(self):\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "def train_agent(env, agent, episodes=1000):\n",
    "    print(\"--- Starting Training ---\")\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update_q_table(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        agent.decay_exploration()\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}/{episodes} | Total Reward: {total_reward}\")\n",
    "    print(\"--- Training Finished ---\")\n",
    "\n",
    "\n",
    "def evaluate_agent(env, agent, episodes=5):\n",
    "    print(\"\\n--- Evaluating Agent ---\")\n",
    "    agent.epsilon = 0  # Turn off exploration for evaluation\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        path = [state]\n",
    "        print(f\"\\n--- Evaluation Episode {episode + 1} ---\")\n",
    "        env.render()\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            path.append(state)\n",
    "            time.sleep(0.5)\n",
    "            env.render()\n",
    "\n",
    "        print(f\"Path taken: {path}\")\n",
    "        print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Gets environment parameters from the user.\"\"\"\n",
    "    try:\n",
    "        size = int(input(\"Enter grid size (e.g., 5 for a 5x5 grid): \"))\n",
    "        \n",
    "        start_str = input(\"Enter start position (row,col), e.g., 0,0: \")\n",
    "        start = tuple(map(int, start_str.split(',')))\n",
    "        \n",
    "        goal_str = input(f\"Enter goal position (row,col), e.g., {size-1},{size-1}: \")\n",
    "        goal = tuple(map(int, goal_str.split(',')))\n",
    "        \n",
    "        obstacles = []\n",
    "        print(\"Enter obstacle positions (row,col), one per line. Type 'done' when finished.\")\n",
    "        while True:\n",
    "            obs_str = input(\"> \")\n",
    "            if obs_str.lower() == 'done':\n",
    "                break\n",
    "            obstacles.append(tuple(map(int, obs_str.split(','))))\n",
    "            \n",
    "        return size, start, goal, obstacles\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please use the correct format.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get environment configuration from the user\n",
    "    size, start, goal, obstacles = get_user_input()\n",
    "\n",
    "    if size is not None:\n",
    "        # 1. Define the Custom Environment\n",
    "        environment = GridWorld(size=size, start=start, goal=goal, obstacles=obstacles)\n",
    "        \n",
    "        # 2. Select and Implement an RL Algorithm (Q-learning)\n",
    "        state_space_size = (size, size)\n",
    "        action_space_size = len(environment.actions)\n",
    "        rl_agent = QLearningAgent(state_space_size, action_space_size)\n",
    "        \n",
    "        # 3. Train the Agent\n",
    "        train_agent(environment, rl_agent, episodes=2000)\n",
    "        \n",
    "        # 4. Evaluate the performance\n",
    "        evaluate_agent(environment, rl_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d94fd6-9996-45f2-a253-a5fba7a8b9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
